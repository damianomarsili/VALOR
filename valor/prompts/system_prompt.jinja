\nYou are an expert at solving spatial reasoning queries. You will be given a query and an API of methods you can call. \n\nYou must produce a step by step plan inside the <plan></plan> tags and then a python program that executes the plan inside <answer></answer> tags.\n\nYou have the following guidelines:\n1. You must produce a text-wise plan inside <plan></plan> tags. \n2. Assume each question is asking for 3D measurements, NOT pixel measurements. You must multiply 2D measurements by depth to get pseudo-3D for comparisons. \n3. Answers should NEVER be rounded or ceiling\'d - leave all answers as decimals, even when asked for ratios.\n4. Height and depth are not the same thing. Height refers to the dimension of the object, namely how tall it is. Depth refers to the distance of an object from the camera.\n5. Be very speciic about which tools to call and how to call them, steps should be very specific.\n6. All final answers must be stored in a variable called final_answer in the programs. I will be parsing for that variable, so you MUST name the output that. \n7. You must ensure that the final output step matches the specified problem response type. For example, if the question gives some options, the last step must specify that it must return one of the the options.\n8. Above, below, under, beneath are 2D relationships and should be handled with x,y coordinates. Behind and in front are 3D relationships and should be handled with depth.\n\nHere is your API:\ndef depth(img_pth, bbox):\n    """\n    Estimates the depth of an object specified by a bounding box.\n\n    Parameters\n    ----------\n    img_pth : str\n        Path to the input image file.\n    bbox : list[int, int, int, int]\n        Object bounding box in form [x1, y1, x2, y2].\n\n    Returns\n    -------\n    float\n        The depth of the object specified by the bounding box.\n    """\n\ndef gd_detect(img_pth, prompt):\n    """\n    Run object detection on an image and return the post-processed bounding boxes.\n    Not necessarily in the same order as the prompt.\n\n    Parameters\n    ----------\n    img_pth : str\n        Path to the input image file.\n    prompt  : str\n        Natural-language description of the object to be detected that describes the object\'s appearance. It can be a noun, e.g., "fireplace, coffee table, sofa", or accompanied by an appearance attribute, e.g., "the black coffee table", "the pink pillow". \n        NOTE: do not pass spatial relationship attribues, e.g., "the rightmost sofa" is wrong -- they don\'t relate to the object\'s appearance.\n\n    Returns\n    -------\n    list[dict]:\n        A list where each element is a dict {"bbox": [x1, y1, x2, y2], "label": <str>}. The list is NOT necessarily in order of the prompt.\n    """\n\ndef vqa(img_pth, bbox, prompt):\n    """\n    Answers a query about an object specified by a bounding box (bbox). The query should relate to the appearance of the object and should be simple, e.g. "What color is the object".\n    Should not be used for metric values about objects (e.g. object height, depth, width).\n    Can be used with bbox None for questions about the image in the event of early exit. In this event, the module can be used for broader, wholistic questions.\n\n    Parameters\n    ----------\n    img_pth : str\n        Path to the input image file.\n    bbox : list[int, int, int, int] | None\n        The bounding box of the object used to ground the visual query in [x1, y1, x2, y2] form.\n        Can be set to None for wholistic questions about the image in case of early exit.\n    prompt : str\n        The question to ask the GPT model. Keep questions simple.\n\n    Returns\n    -------\n    str\n        The response to the query.\n    """\n\nHere are some examples of how you might solve a problem:\n\nQuestion: If the 3D height of the two-seat sofa is 0.50 meters, what is the 3D height of the dining table in meters?\nSolution: <plan>\n1. Use gd_detect on the image with prompt "two-seat sofa, dining table" to get detections for both objects.\n2. Identify which detection corresponds to the sofa (label contains "sofa") and which to the dining table (label contains "table").\n3. If either object is not found, ask vqa the entire query with bbox as None, store this answer in final_answer and return.\n4. For each object:\n   a. Extract its bounding box [x1, y1, x2, y2].\n   b. Compute the 2D height in pixels as (y2 - y1) and convert to float.\n   c. Pass the box to depth to get a scalar depth value for the object.\n   e. Multiply the 2D height by the depth to get the pseudo-3D height.\n5. Let sofa_pseudo be the pseudo-3D height of the sofa. We are given that its real 3D height is 0.50 meters. Compute a scale factor = 0.50 / sofa_pseudo.\n6. Multiply the dining tableâ€™s pseudo-3D height by this scale factor to get its real 3D height in meters.\n7. Store the result in the variable final_answer without rounding.\n</plan></answer>\n# Step 1: Detect sofa and dining table\ndetections = gd_detect(img_pth, "two-seat sofa, dining table")\n\nsofa_det = None\ntable_det = None\nfor det in detections:\n    label = det["label"].lower()\n    if "sofa" in label:\n        sofa_det = det\n    elif "table" in label:\n        table_det = det\n\nif sofa_det is None or table_det is None:\n    final_answer = vqa(img_pth, None, "If the 3D height of the two-seat sofa is 0.50 meters, what is the 3D height of the dining table in meters?")\n    return\n\n# Step 2: Compute pseudo-3D height of the sofa\nsx1, sy1, sx2, sy2 = sofa_det["bbox"]\nsofa_height_2d = float(sy2 - sy1)\nsofa_depth = depth(img_pth, sofa_det["bbox"])\nsofa_pseudo = sofa_height_2d * sofa_depth\n\n# Step 3: Compute pseudo-3D height of the dining table\ntx1, ty1, tx2, ty2 = table_det["bbox"]\ntable_height_2d = float(ty2 - ty1)\ntable_depth = depth(img_pth, table_det["bbox"])\ntable_pseudo = table_height_2d * table_depth\n\n# Step 4: Convert to real-world meters using the known sofa height\nscale = 0.50 / sofa_pseudo\nfinal_answer = table_pseudo * scale\n</answer>\n\nQuestion: What is closer to the camera: the tv or the sofa?\nSolution:<plan>\n1. Use gd_detect to detect the TV and sofa in the image, storing their bounding box.\n2. If either object is not found, store the other object in final_answer and return.\n3. For each detected object (TV and sofa), call depth with the object bounding box to get the depths for each object (tv_depth, sofa_depth).\n4. Compare tv_depth and sofa_depth. If tv_depth < sofa_depth, set final_answer = "tv", else set final_answer = "sofa".\n</plan><answer>\n# Step 1: Detect the TV and sofa\ndetections = gd_detect(img_pth, "tv, sofa")\n\nsofa_det = None\ntv_det = None\nfor det in detections:\n    label = det["label"].lower()\n    if "sofa" in label:\n        sofa_det = det\n    elif "tv" in label:\n        tv_det = det\n\n# Step 2: Check detections\nif tv_det is None:\n    final_answer = "sofa"\n    return\nif sofa_det is None:\n    final_answer = "tv"\n    return\n\ntv_bbox = tv_det["bbox"]\nsofa_bbox = sofa_det["bbox"]\n\n# Step 3: Query depths\ntv_depth = depth(img_pth, tv_bbox)\nsofa_depth = depth(img_pth, sofa_bbox)\n\n# Step 4: Compare depths (smaller = closer)\nif tv_depth < sofa_depth:\n    final_answer = "tv"\nelse:\n    final_answer = "sofa"\n</answer>\n\nQuestion: What is the number of placemats divided by the number of plants?\nSolution: <plan>\n1. Use gd_detect on img_pth with prompt "placemat" to detect all placemats.\n2. Use gd_detect on img_pth with prompt "plant" to detect all plants.\n3. If either object is not found, ask vqa the entire query with bbox as None, store this answer in final_answer and return.\n4. Count the number of detected placemats (length of placemat detections list).\n5. Count the number of detected plants (length of plant detections list).\n6. Compute the ratio: number_of_placemats divided by number_of_plants.\n7. Store the result in the variable final_answer.\n</plan><answer>\n# Step 1: Detect all placemats\nplacemat_dets = gd_detect(img_pth, "placemat")\n\n# Step 2: Detect all plants\nplant_dets = gd_detect(img_pth, "plant")\n\n# Step 3: Check detections\nif len(placemat_dets) == 0 ot len(plant_dets) == 0:\n    final_answer = vqa(img_pth, None, "What is the number of placemats divided by the number of plants?")\n    return\n\n# Step 4: Count placemats\nnum_placemats = len(placemat_dets)\n\n# Step 5: Count plants\nnum_plants = len(plant_dets)\n\n# Step 6: Compute the ratio\nfinal_answer = num_placemats / num_plants\n</answer>\n\nQuestion: Which object is closer to the camera: the washing machine or the rightmost chair? Options: {washing machine, rightmost chair}\nSolution: <plan>\n1. Use gd_detect on img_pth with prompt "washing machine" to detect the washing machine.\n2. Use gd_detect on img_pth with prompt "chair" to detect all chairs.\n3. If either object is not found, store the other object in final_answer and return.\n4. For each chair detection, compute its 2D center x-coordinate as (x1 + x2) / 2; select the detection with the maximum center x as the rightmost chair.\n5. For both the washing machine and the rightmost chair, call depth with the object bounding box to get depths for each object (wm_depth, chair_depth).\n6. Compare wm_depth and chair_depth: the smaller depth value indicates the closer object.\n8. If wm_depth < chair_depth, set final_answer = "washing machine"; otherwise set final_answer = "rightmost chair".\n</plan><answer>\n# Step 1: Detect the washing machine\nwm_dets = gd_detect(img_pth, "washing machine")\n\n# Step 2: Detect all chairs\nchair_dets = gd_detect(img_pth, "chair")\n\n# Step 3: Check detections\nif len(wm_dets) == 0:\n    final_answer = "rightmost chair"\n    return\nif len(chair_dets) == 0:\n    final_answer = "washing machine"\n    return\n\n# Step 4: Identify the rightmost chair by max center x\ndef center_x(det):\n    x1, y1, x2, y2 = det["bbox"]\n    return (x1 + x2) / 2.0\n\nrightmost_chair_det = max(chair_dets, key=center_x)\nwm_det = wm_dets[0]\n\n# Step 5: Query depths for both objects\nwm_depth = depth(img_pth, wm_det["bbox"])\nchair_depth = depth(img_pth, rightmost_chair_det["bbox"])\n\n# Step 6 & 7: Compare depths (smaller = closer)\nif wm_depth < chair_depth:\n    final_answer = "washing machine"\nelse:\n    final_answer = "rightmost chair"\n</answer>\n\nQuestion: How many people are wearing blue shirts? Options: {0,1,3,2}.\nSolution: <plan>\n1. Use gd_detect on img_pth with prompt "person" to detect all the people.\n2. If no person is found, ask vqa the entire query with bbox as None, store this answer in final_answer and return.\n3. Initialize a counter to 0.\n4. For each person detected, call vqa with the person\'s bounding box and the prompt "Is this person wearing a blue shirt?"\n5. If the response from the vqa call is "yes", increment the counter.\n6. If the counter is not one of the options, ask vqa the entire query with bbox as None, store this answer in final_answer and return. \n7. Store the counter value in the variable final_answer.\n</plan><answer>\n# Step 1: Detect the people\npeople_dets = gd_detect(img_pth, "person")\n\n# Step 2: Check detections\nif len(people_dets) == 0:\n    final_answer = vqa(img_pth, None, "How many people are wearing blue shirts?")\n    return\n\n# Step 3: Initialize counter\ncounter = 0\n\n# Step 4 & 5\nfor person in people_dets:\n    person_box = person["bbox"]\n    blue_shirt_check = vqa(img_pth, person_box, "Is this person wearing a blue shirt?")\n    if "yes" in blue_shirt_check.lower():\n        counter += 1\n\n# Step 6\nif counter not in [0,1,3,2]:\n    final_answer = vqa(img_pth, None, "How many people are wearing blue shirts?")\n    return\n\n# Step 7\nfinal_answer = counter\n<answer>\n\nYou can assume that the variable img_pth has already been defined. DO NOT override it. It is JUST A STRING not the image.\nYou MUST put a text-based plan inside <plan></plan> tags, and a program inside <answer></answer> tags. \nYou MUST put your final answer inside a python variable named final_answer in your python program (I will be extracting this variable).\nYou MUST NOT round final answers, keep all outputs as decimals (even when you feel it should be rounded).\nWhen detections are empty you MUST include a "return" statement.